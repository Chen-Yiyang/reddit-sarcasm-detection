{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS3244 Project - Linh.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GBT09mqpiPLK"},"source":["## **CS3244 Project - Models**"]},{"cell_type":"code","metadata":{"id":"Olcmg2t28XWn","executionInfo":{"status":"ok","timestamp":1637134498303,"user_tz":-480,"elapsed":1027,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.model_selection import GridSearchCV"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1UylM6d1KaS","executionInfo":{"status":"ok","timestamp":1637134498304,"user_tz":-480,"elapsed":5,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["# General function for running any model\n","def run(X_train, y_train, X_test, y_test, model):\n","  model.fit(X_train, y_train)\n","  X_train_predict = model.predict(X_train)\n","  X_test_predict = model.predict(X_test)\n","  train_score_roc = roc_auc_score(y_train, X_train_predict)\n","  test_score_roc = roc_auc_score(y_test, X_test_predict)\n","  train_score = precision_recall_fscore_support(y_train, X_train_predict, average='micro')\n","  test_score = precision_recall_fscore_support(y_test, X_test_predict, average='micro')\n","  print(f\"Training performance (ROC-AUC): {train_score_roc}\")\n","  print(f\"Test performance (ROC-AUC): {test_score_roc}\")\n","  print(f\"Training performance (Precision - Recall - F1): {train_score}\")\n","  print(f\"Test performance (Precision - Recall - F1): {test_score}\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"iS8guXTHZZW0","executionInfo":{"status":"ok","timestamp":1637134499369,"user_tz":-480,"elapsed":2,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["# Display best hyperparameters for a given model\n","def tune(X_train, y_train, X_test, y_test, model, hyperparameters):\n","  clf = GridSearchCV(model, hyperparameters)\n","  clf.fit(X_train, y_train)\n","  print(\"Best hyperparameters: \" + str(clf.best_params_))\n","\n","  # Run the model with the best params\n","  model.set_params(**clf.best_params_)\n","  run(X_train, y_train, X_test, y_test, model)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ukWxEJE_046S"},"source":["## Import Data"]},{"cell_type":"code","metadata":{"id":"sgeXMs-Z8Hql","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637128791546,"user_tz":-480,"elapsed":24702,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}},"outputId":"498be4d3-81da-4ef0-b562-73d35fd472dc"},"source":["# Run this only when you are using Google Colab\n","! pip install kaggle\n","! mkdir ~/.kaggle\n","! cp kaggle.json ~/.kaggle/\n","! chmod 600 ~/.kaggle/kaggle.json\n","! kaggle datasets download danofer/sarcasm\n","! unzip sarcasm\n","! kaggle datasets download chenyiyang/reddit-sarcasm-extracted-features-dataset\n","! unzip reddit-sarcasm-extracted-features-dataset"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n","Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n","Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n","Downloading sarcasm.zip to /content\n"," 99% 214M/216M [00:01<00:00, 150MB/s]\n","100% 216M/216M [00:01<00:00, 149MB/s]\n","Archive:  sarcasm.zip\n","  inflating: test-balanced.csv       \n","  inflating: test-unbalanced.csv     \n","  inflating: train-balanced-sarc.csv.gz  \n","  inflating: train-balanced-sarcasm.csv  \n","Downloading reddit-sarcasm-extracted-features-dataset.zip to /content\n","100% 14.2M/14.2M [00:00<00:00, 45.4MB/s]\n","\n","Archive:  reddit-sarcasm-extracted-features-dataset.zip\n","  inflating: X-test-v1.0.csv         \n","  inflating: X-train-v1.0.csv        \n","  inflating: y-test-v1.0.csv         \n","  inflating: y-train-v1.0.csv        \n"]}]},{"cell_type":"code","metadata":{"id":"d15gCy7Qek0T"},"source":["# Run if you are using this notebook offline\n","# ! /Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install pandas\n","# ! /Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install sklearn\n","# ! /Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install keras\n","# ! /Library/Frameworks/Python.framework/Versions/3.9/bin/python3 -m pip install tensorflow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjdcQN5HlwO_","executionInfo":{"status":"ok","timestamp":1637134511021,"user_tz":-480,"elapsed":9373,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["# Read data\n","df = pd.read_csv('train-balanced-sarcasm.csv')\n","df['comment'].replace('', np.nan, inplace=True)\n","df.dropna(subset=['comment'], inplace=True)\n","X, y = df['comment'].values, df['label'].values\n","\n","# Split into train (size 808618) and test (size 202155)\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ce9owDgLhih"},"source":["## Generate Input Vectors"]},{"cell_type":"code","metadata":{"id":"RlXX8IWEnn2Y","executionInfo":{"status":"ok","timestamp":1637134511022,"user_tz":-480,"elapsed":9,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","def to_count(X_train, X_test):\n","  vectorizer = CountVectorizer()\n","  return vectorizer.fit_transform(X_train), vectorizer.transform(X_test)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gmf_1D9tME35","executionInfo":{"status":"ok","timestamp":1637134511023,"user_tz":-480,"elapsed":8,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_selection import SelectKBest, f_classif\n","\n","MIN_DF = 2\n","TOP_K = 1000000\n","BEST_NGRAM_RANGE = (1, 3)\n","\n","def to_tfidf(X_train, X_test, ngram_range):\n","  vectorizer = TfidfVectorizer(ngram_range=ngram_range, min_df=MIN_DF, \n","    dtype=np.float64, strip_accents='unicode', decode_error='replace')\n","  X_train_tfidf = vectorizer.fit_transform(X_train)\n","  X_test_tfidf = vectorizer.transform(X_test)\n","\n","  # Set a limit on the number of features in a vector\n","  selector = SelectKBest(f_classif, k=min(TOP_K, X_train_tfidf.shape[1]))\n","  selector.fit(X_train_tfidf, y_train)\n","  X_train_tfidf = selector.transform(X_train_tfidf).astype('float32')\n","  X_test_tfidf = selector.transform(X_test_tfidf).astype('float32')\n","  return X_train_tfidf, X_test_tfidf"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"wXE48lVjC6i3","executionInfo":{"status":"ok","timestamp":1637134520477,"user_tz":-480,"elapsed":9461,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["# vocab_size = 159401\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","def to_keras_embed(X_train, X_test):\n","  tokenizer = Tokenizer(num_words=160000)\n","  tokenizer.fit_on_texts(X_train)\n","  X_train_embed = tokenizer.texts_to_sequences(X_train)\n","  X_test_embed = tokenizer.texts_to_sequences(X_test)\n","  vocab_size = len(tokenizer.word_index) + 1\n","\n","  # Pad with zeros so each embedding vector is equal length\n","  X_train_embed = pad_sequences(X_train_embed, padding='post', maxlen=200)\n","  X_test_embed = pad_sequences(X_test_embed, padding='post', maxlen=200)\n","  return X_train_embed, X_test_embed"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTKZd3QZHWEm","executionInfo":{"status":"ok","timestamp":1637134521539,"user_tz":-480,"elapsed":1067,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}},"outputId":"79edd63a-0a77-4e7c-9dbe-9434eb519ddf"},"source":["import nltk\n","nltk.download('punkt')\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from nltk.tokenize import word_tokenize\n","\n","def to_dtv_embed(X_train, X_test):\n","  # Implemented based on https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5\n","  tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(X_train)]\n","  model = Doc2Vec(size=250, alpha=0.025, min_alpha=0.00025, min_count=1, dm=1)\n","  model.build_vocab(tagged_data)\n","  model.train(tagged_data, total_examples=model.corpus_count, epochs=model.iter)\n","  model.alpha -= 0.0002\n","  model.min_alpha = model.alpha\n","  X_train_dtv = [model.infer_vector(word_tokenize(comment.lower())) for comment in X_train]\n","  X_test_dtv = [model.infer_vector(word_tokenize(comment.lower())) for comment in X_test]\n","  return X_train_dtv, X_test_dtv"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"lKkVMw2We-ER"},"source":["X_train_counts, X_test_counts = to_count(X_train, X_test)\n","X_train_tfidf, X_test_tfidf = to_tfidf(X_train, X_test, BEST_NGRAM_RANGE)\n","X_train_keras, X_test_keras = to_keras_embed(X_train, X_test)\n","X_train_features, X_test_features = np.genfromtxt(\"X-train-v1.0.csv\", delimiter=','), np.genfromtxt(\"X-test-v1.0.csv\", delimiter=',')\n","# X_train_dtv, X_test_dtv = to_dtv_embed(X_train, X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLgXSNfWB7ge"},"source":["## **Convolutional Neural Network (CNN)**\n","\n","We implement a Convolutional Neural Network (CNN) model with reference to [this article](https://analyticsindiamag.com/guide-to-text-classification-using-textcnn/)."]},{"cell_type":"code","metadata":{"id":"R3ueUsW-u90G","executionInfo":{"status":"ok","timestamp":1637134521539,"user_tz":-480,"elapsed":4,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["# Create CNN model\n","from keras.models import Sequential\n","from keras import layers"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zWTvICZrZAaq"},"source":["# Evaluation"]},{"cell_type":"code","metadata":{"id":"rZsZz9MFZAA5","executionInfo":{"status":"ok","timestamp":1637134521540,"user_tz":-480,"elapsed":4,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["# Get a CNN model\n","def get_cnn_model(epoch, vocab_size, embedding_dim, maxlen):\n","  textcnnmodel = Sequential()\n","  textcnnmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n","  textcnnmodel.add(layers.Conv1D(256, 5, activation='relu'))\n","  textcnnmodel.add(layers.GlobalMaxPooling1D())\n","  textcnnmodel.add(layers.Dense(10, activation='relu'))\n","  textcnnmodel.add(layers.Dense(1, activation='sigmoid'))\n","  textcnnmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","  textcnnmodel.summary()\n","  return textcnnmodel"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"8JVDC8hRTwgN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637134583368,"user_tz":-480,"elapsed":61571,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}},"outputId":"c4658711-4c4e-4a92-e306-fe473e4e744b"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import gensim\n","from gensim.models import Word2Vec, KeyedVectors\n","\n","# creating corpus\n","corpus_text = 'n'.join(df['comment'])\n","data = []\n","# iterate through each sentence in the file\n","for i in sent_tokenize(corpus_text):\n","    temp = []\n","    # tokenize the sentence into words\n","    for j in word_tokenize(i):\n","        temp.append(j.lower())\n","    data.append(temp)\n","\n","# Building word2vec model using Gensim\n","#CBOW\n","# model1 = gensim.models.Word2Vec(data, min_count=1, size=100, window=5, sg=0)\n","#skip-gram\n","# model2 = gensim.models.Word2Vec(data, min_count=1, size=100, window=5, sg=1)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"p5o8LE81lv0e"},"source":["Can consider using context + doc2vec as stated in article:\n","https://medium.com/@namanjain2050/using-deep-learning-to-identify-sarcasm-100a4a4ceaea"]},{"cell_type":"code","metadata":{"id":"gWyfvpErd8Ms","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637134586891,"user_tz":-480,"elapsed":3530,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}},"outputId":"45d7ae2f-91ac-435c-b0b2-fbae895c70cc"},"source":["# getting Google pretrained W2V\n","!pip install wget==3.2\n","from keras.models import load_model\n","\n","import os\n","import wget\n","import gzip\n","import shutil\n","\n","gn_vec_path = \"GoogleNews-vectors-negative300.bin\"\n","if not os.path.exists(\"GoogleNews-vectors-negative300.bin\"):\n","    if not os.path.exists(\"../Ch2/GoogleNews-vectors-negative300.bin\"):\n","        #Downloading the reqired model\n","        if not os.path.exists(\"../Ch2/GoogleNews-vectors-negative300.bin.gz\"):\n","            if not os.path.exists(\"GoogleNews-vectors-negative300.bin.gz\"):\n","                wget.download(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\")\n","            gn_vec_zip_path = \"GoogleNews-vectors-negative300.bin.gz\"\n","        else:\n","            gn_vec_zip_path = \"../Ch2/GoogleNews-vectors-negative300.bin.gz\"\n","        #Extracting the required model\n","        with gzip.open(gn_vec_zip_path, 'rb') as f_in:\n","            with open(gn_vec_path, 'wb') as f_out:\n","                shutil.copyfileobj(f_in, f_out)\n","    else:\n","        gn_vec_path = \"../Ch2/\" + gn_vec_path\n","\n","print(f\"Model at {gn_vec_path}\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: wget==3.2 in /usr/local/lib/python3.7/dist-packages (3.2)\n","Model at GoogleNews-vectors-negative300.bin\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WqYDM8pCcMP2","executionInfo":{"status":"ok","timestamp":1637134623631,"user_tz":-480,"elapsed":36745,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}},"outputId":"80ca34bb-b57e-4d01-f415-20df89aa069c"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","import gensim\n","from gensim.models import Word2Vec, KeyedVectors\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","train_bal = df.sample(frac=0.8)\n","remaining = df.drop(train_bal.index)\n","cv_bal = remaining.sample(frac=0.5)\n","test_bal = remaining.drop(cv_bal.index)\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(train_bal['comment'].values)\n","\n","encoded_comments_train = tokenizer.texts_to_sequences(train_bal['comment'])\n","encoded_comments_cv = tokenizer.texts_to_sequences(cv_bal['comment'])\n","encoded_comments_test = tokenizer.texts_to_sequences(test_bal['comment'])\n","\n","vocab_size = len(tokenizer.word_index) + 1\n","print(vocab_size)\n","\n","padded_comments_train = pad_sequences(encoded_comments_train, maxlen=200, padding='post')\n","padded_comments_cv = pad_sequences(encoded_comments_cv, maxlen=200, padding='post')\n","padded_comments_test = pad_sequences(encoded_comments_test, maxlen=200, padding='post')\n","\n","y_train = train_bal['label'].values\n","y_cv = cv_bal['label'].values\n","y_test = test_bal['label'].values\n","\n","y_train = to_categorical(y_train, num_classes=2)\n","y_cv = to_categorical(y_cv, num_classes=2)\n","y_test = to_categorical(y_test, num_classes=2)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","159478\n"]}]},{"cell_type":"code","metadata":{"id":"GollLfmQeQ_J","executionInfo":{"status":"ok","timestamp":1637134721782,"user_tz":-480,"elapsed":98156,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}}},"source":["#loading our W2V pre-trained vectors\n","w2v_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n","\n","# create a weight matrix for words in dictionary\n","embedding_matrix_w2v = np.zeros((vocab_size, 300))\n","for word, i in tokenizer.word_index.items():\n","    try:\n","        embedding_vector = w2v_model[word]\n","    except:\n","        embedding_vector = [0]*300\n","    \n","    if embedding_vector is not None:\n","        embedding_matrix_w2v[i] = embedding_vector"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJFlgaIhfGR9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637135610731,"user_tz":-480,"elapsed":449,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}},"outputId":"8ae218f9-b69a-43d8-ee51-90d098f2887a"},"source":["from keras import Input, layers, Model, callbacks, optimizers\n","import tensorflow as tf\n","from keras import backend as K\n","\n","# model definition - this model is purely based on content\n","\n","def recall_m(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","        recall = true_positives / (possible_positives + K.epsilon())\n","        return recall\n","\n","def precision_m(y_true, y_pred):\n","        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","        precision = true_positives / (predicted_positives + K.epsilon())\n","        return precision\n","\n","def f1_m(y_true, y_pred):\n","        precision = precision_m(y_true, y_pred)\n","        recall = recall_m(y_true, y_pred)\n","        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","\n","input_data = Input(shape=(200,), name='main_input')\n","embedding_layer = layers.Embedding(vocab_size, 300, weights=[embedding_matrix_w2v])(input_data)\n","conv_1 = layers.Conv1D(filters=128, kernel_size=4, activation='relu')(embedding_layer)\n","max_1 = layers.MaxPooling1D(pool_size=2)(conv_1)\n","conv_2 = layers.Conv1D(filters=64, kernel_size=3, activation='relu')(max_1)\n","max_2 = layers.MaxPooling1D(pool_size=2)(conv_2)\n","flatten = layers.Flatten()(max_2)\n","dense = layers.Dense(100, activation='relu', name='fully_connected')(flatten)\n","out = layers.Dense(2, activation='softmax')(dense)\n","\n","model_01 = Model(inputs=[input_data], outputs=[out])\n","\n","print(model_01.summary())\n","\n","#defining checkpoints\n","tensorboard = callbacks.TensorBoard(log_dir='model_01')\n","\n","reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_f1_m', \n","                              mode = 'max', \n","                              factor=0.5, \n","                              patience=5, \n","                              min_lr=0.0001, \n","                              verbose=10)\n","\n","checkpoint = callbacks.ModelCheckpoint(\"model_01.h5\", \n","                               monitor=\"val_f1_m\", \n","                               mode=\"max\", \n","                               save_best_only = True, \n","                               verbose=1)\n","\n","earlystop = callbacks.EarlyStopping(monitor = 'val_f1_m', \n","                            mode=\"max\", \n","                            min_delta = 0, \n","                            patience = 2,\n","                            verbose=1)\n","\n","#compiling model\n","c = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n","model_01.compile(optimizer=c, loss='categorical_crossentropy', metrics=['acc', f1_m])"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," main_input (InputLayer)     [(None, 200)]             0         \n","                                                                 \n"," embedding_4 (Embedding)     (None, 200, 300)          47843400  \n","                                                                 \n"," conv1d_8 (Conv1D)           (None, 197, 128)          153728    \n","                                                                 \n"," max_pooling1d_8 (MaxPooling  (None, 98, 128)          0         \n"," 1D)                                                             \n","                                                                 \n"," conv1d_9 (Conv1D)           (None, 96, 64)            24640     \n","                                                                 \n"," max_pooling1d_9 (MaxPooling  (None, 48, 64)           0         \n"," 1D)                                                             \n","                                                                 \n"," flatten_4 (Flatten)         (None, 3072)              0         \n","                                                                 \n"," fully_connected (Dense)     (None, 100)               307300    \n","                                                                 \n"," dense_4 (Dense)             (None, 2)                 202       \n","                                                                 \n","=================================================================\n","Total params: 48,329,270\n","Trainable params: 48,329,270\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"code","metadata":{"id":"Tz08u9QQqQou","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637139273281,"user_tz":-480,"elapsed":2174930,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}},"outputId":"a3fa5049-e855-4a51-b9f8-ff095b2ca65f"},"source":["#training\n","h1 = model_01.fit(padded_comments_train, y_train, \n","               batch_size=64,\n","               epochs=10, \n","               verbose=1, callbacks=[tensorboard, checkpoint, earlystop, reduce_lr], \n","               validation_data=(padded_comments_cv, y_cv))"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","12635/12635 [==============================] - ETA: 0s - loss: 0.4173 - acc: 0.8095 - f1_m: 0.8095\n","Epoch 00001: val_f1_m did not improve from 0.72577\n","12635/12635 [==============================] - 722s 57ms/step - loss: 0.4173 - acc: 0.8095 - f1_m: 0.8095 - val_loss: 0.5879 - val_acc: 0.7108 - val_f1_m: 0.7109 - lr: 1.0000e-04\n","Epoch 2/10\n","12635/12635 [==============================] - ETA: 0s - loss: 0.3503 - acc: 0.8476 - f1_m: 0.8476\n","Epoch 00002: val_f1_m did not improve from 0.72577\n","12635/12635 [==============================] - 723s 57ms/step - loss: 0.3503 - acc: 0.8476 - f1_m: 0.8476 - val_loss: 0.6450 - val_acc: 0.7055 - val_f1_m: 0.7056 - lr: 1.0000e-04\n","Epoch 3/10\n","12635/12635 [==============================] - ETA: 0s - loss: 0.2852 - acc: 0.8802 - f1_m: 0.8802\n","Epoch 00003: val_f1_m did not improve from 0.72577\n","12635/12635 [==============================] - 730s 58ms/step - loss: 0.2852 - acc: 0.8802 - f1_m: 0.8802 - val_loss: 0.7253 - val_acc: 0.6941 - val_f1_m: 0.6942 - lr: 1.0000e-04\n","Epoch 00003: early stopping\n"]}]},{"cell_type":"code","metadata":{"id":"7D4BcWMgfPT8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637139342374,"user_tz":-480,"elapsed":69098,"user":{"displayName":"Linh Cao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11483750052617879683"}},"outputId":"81a4a6d5-ebaf-4ea3-9719-4c0c54ae31bc"},"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","score_1 = model_01.evaluate(padded_comments_test, y_test, verbose=True)\n","print(score_1)\n","\n","cnf_mat = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(model_01.predict(padded_comments_test), axis=1))\n","\n","print(cnf_mat)\n","# sns.heatmap(cnf_mat, annot=True, fmt='g', linewidths=.5, xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n","            \n","# plt.plot(h1.history['f1_m'][1:])\n","# plt.plot(h1.history['val_f1_m'][1:])\n","# plt.title('model iou metric')\n","# plt.ylabel('F1 metric')\n","# plt.xlabel('epoch')\n","# plt.legend(['train','Validation'], loc='upper left')\n","# plt.show()"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["3159/3159 [==============================] - 24s 8ms/step - loss: 0.7140 - acc: 0.6980 - f1_m: 0.6979\n","[0.7139533162117004, 0.6979530453681946, 0.6979132890701294]\n","[[34411 16293]\n"," [14237 36136]]\n"]}]}]}